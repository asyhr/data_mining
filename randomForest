#!/usr/bin/env python
# coding: utf-8

import numpy as np
import scipy as sc
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import importlib
importlib.import_module('mpl_toolkits.mplot3d').Axes3D
import seaborn as sns
# from IPython.display import display
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
# import decision tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import random
import re

random.seed(75)

def readData():
    # data preparation
    # get the dataset from file
    featureFile = pd.read_csv("C:/Users/amira/OneDrive/Documents/msc_data_sc/scc403/Project/WLA.csv", header=None)
    labelsFile = pd.read_csv("C:/Users/amira/OneDrive/Documents/msc_data_sc/scc403/Project/Labels.csv", header=None)


    # combine feature and labels datasets into one
    dataset = pd.concat([featureFile, labelsFile], axis=1)

    # shuffle the dataframe because the first 16 lines
    # has the same class label. Which will not be good for
    # training and validation later
    data = dataset.sample(frac=1)
    # separate it back after shuffle it
    features = data.iloc[:, :-1]
    classes = data.iloc[:, -1]
    return features, classes

def preProcess(features, labels):
    #check for missing data
    print('Check for Features Missing Data: ')
    print(features.isnull().sum())
    print('Check for Labels Missing Data: ')
    print(labels.isnull().sum())
    # pre-processing
    # normalize dataset to 0 - 1 scale
    # need to do scaling because I am using svm algorithms
    # svm fit a model by using weighted sum of input variables (svm use distance measures between
    # example or examplars)

    # decision tree does not affected by the scaling of input variables
    # https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/
    # we only use normalization and not standardization
    # no standardization bcs our data are not using different scales like minutes, hours, meters
    # they all use pixel to calculate the wla, so tak payah standardize
    # just normalized so the value is withing 0 - 1, senang nak work with
    scaler = MinMaxScaler()
    # transform data
    features2 = scaler.fit_transform(features)
    # print(scaled)
    # print(data)
    #print(features)
    # print(classes)
    print('The first five rows of the original features: ')
    print(features)
    print('The first five rows of the normalised features: ')
    print(features2)
    print('The first five rows of the class labels: ')
    print(labels)
    return features2


def gridSearch(clf,xt, yt, pr, xtest, ytest):
    #talk about cross-validation
    #use random grid to search for the best hyperparameters
    #create base model for parameter tuning
    #rf1 = RandomForestClassifier()


    rf_cv = GridSearchCV(clf, pr, cv = 5, scoring = 'accuracy')
    rf_cv.fit(xt, yt)
    rf_cv_pd = pd.DataFrame(rf_cv.cv_results_)
    print('Best Parameters: ', rf_cv.best_params_)

    print(rf_cv_pd.sort_values(['rank_test_score'], ascending=True).reset_index(drop=True).head(8))
    best_score = (rf_cv.best_score_) * 100
    print('Accuracy of Trained Model: ', best_score, '%')

    # test the data random forest
    # use the parameter suggested by gridsearchcv
    clf_rf1 = RandomForestClassifier(**rf_cv.best_params_)
    clf_rf1.fit(xt, yt)
    y_pred1 = clf_rf1.predict(xtest)
    accuracy1 = accuracy_score(ytest, y_pred1) * 100
    print('Accuracy of Test Random Forest Model: ', accuracy1, '%')
    # passing actual and predicted values
    cm1 = confusion_matrix(y_test, y_pred1, labels=clf_rf1.classes_)

    # true Write data values in each cell of the matrix
    sns.heatmap(cm1, annot=True)
    plt.savefig('cm_rf.png')

    # printing the report
    print('Classification Report: ')
    print(classification_report(y_test, y_pred1))
    # print("score on test: " + str(clf_rf1.score(X_test, y_test)))
    # print("score on train: " + str(clf_rf1.score(X_train, y_train)))


clf_rf = RandomForestClassifier()

param = [[{'max_depth': list(range(5, 15)), 'max_features': list(range(1,3)),
           'criterion': ['gini', 'entropy']}]]

clf_labels = ['RF']
clfs = [clf_rf]

features, labels = readData()
features = preProcess(features, labels)
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.20, random_state = 50)

for l, clf, p in zip(clf_labels, clfs, param):
    print('{}:'.format(l))
    gridSearch(clf, X_train, y_train, p, X_test, y_test)
