#!/usr/bin/env python
# coding: utf-8
#this code was run on Jupyter Notebook

import matplotlib
from sklearn.preprocessing import MinMaxScaler
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans
import seaborn as sns
import os
import pandas as pd
import numpy as np
from itertools import cycle, islice
import matplotlib.pyplot as plt
from pandas.plotting import parallel_coordinates
import random
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LassoCV

random.seed(75)

#%matplotlib inline

def readData():
    # data preparation
    # get the dataset from file
    featureFile = pd.read_csv("C:/Users/amira/OneDrive/Documents/msc_data_sc/scc403/Project/ClimateDataBasel.csv", header=None)
    #labelsFile = pd.read_csv("Labels.csv", header=None)
    #EDA
    print('Feature Shape: ')
    print(featureFile.shape)
    print(featureFile.head())

    return featureFile

def preProcessData(features):
    
    #count missing value
    null_val = features.isnull().sum().sort_values(ascending = False)
    print('\nCount Missing Values: ')
    print(null_val)
    
    names = features.columns
    indexes = features.index
    
    #normalize data
    scaler = MinMaxScaler((0,1))
    # transform data
    features = scaler.fit_transform(features)
    features_pd = pd.DataFrame(features)
    
    #statistical dataset
    df = pd.DataFrame(features_pd.describe().transpose())
    print('Feature Statistical Value: ')
    print(df.sort_values(by = ['mean'], ascending = False))
    
    return features, names, indexes, features_pd

features = readData()

features2, names, indexes, ft_pd = preProcessData(features)
data_scaled = pd.DataFrame(features2)

#features3 = pd.DataFrame(features)
ft_pd.head()

features3 = data_scaled.drop(data_scaled.columns[[3,9,10,11,12,13,14,15,16,17]], axis=1)
print(features3)

def outlier_thresholds(dataframe, variable):
    quartile1 = dataframe[variable].quantile(0.01)
    quartile3 = dataframe[variable].quantile(0.99)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit


def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit
    
for col in features3.columns:
    replace_with_thresholds(features3, col)

plt.figure(figsize=(10,10))
sns.boxplot(data=features3)
plt.xticks(rotation=90)
plt.show()

#Building the clustering model and calculating
#the values of the distortion and inertia
kmeans = KMeans()
ssd = []
K = range(1, 30)

for k in K:
    kmeans = KMeans(n_clusters=k).fit(features3)
    ssd.append(kmeans.inertia_)

ssd

plt.plot(K, ssd, "bx-")
plt.xlabel("Distance Residual Sums for K Values (WCSS)")
plt.title("Elbow Method for Optimum Number of Clusters")
plt.show()

kmeans = KMeans()
visu = KElbowVisualizer(kmeans, k=(2, 20))
visu.fit(features3)
visu.show()

kmeans = KMeans(n_clusters=7)
model = kmeans.fit(features2)
print('Model\n: ', model)

centers = model.cluster_centers_
centers

inertias = model.inertia_

# Function that creates a DataFrame with a column for Cluster Number
def pd_centers(featuresUsed, centers):
    colNames = list(featuresUsed)
    colNames.append('prediction')

    # Zip with a column called 'prediction' (index)
    Z = [np.append(A, index) for index, A in enumerate(centers)]

    # Convert to pandas data frame for plotting
    P = pd.DataFrame(Z, columns=colNames)
    P['prediction'] = P['prediction'].astype(int)
    return P

# Function that creates Parallel Plots

def parallel_plot(data):
    my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(data)))
    plt.figure(figsize=(15,8)).gca().axes.set_ylim([-3,+3])
    parallel_coordinates(data, 'prediction', color = my_colors, marker='o')


P = pd_centers(features, centers)
P




